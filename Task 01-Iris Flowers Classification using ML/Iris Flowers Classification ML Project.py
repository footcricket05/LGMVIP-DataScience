# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g-d8aG1qfJh4eeeGa5KAMlnHUMpP8rON
"""

# Commented out IPython magic to ensure Python compatibility.
#Import Libraries/Packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')

"""# **Dataset Description**

The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.

Attribute Information:

1.   Sepal length in cm
2.   Sepal width in cm
1.   Petal length in cm
2.   Petal width in cm

Iris flower can be divided into 3 species as per the length and width of their Sepals and Petals:

1.   Iris Setosa
2.   Iris Versicolour
1.   Iris Virginica


"""

#Loading the dataset
df = pd.read_csv("Iris.csv")
df.head()

# Shape of Dataset
df.shape

# Dataset Columns
df.columns

#Dataset Summary
df.info()

#Dataset Statistical Summary
df.describe()

#Checking Null Values
df.isnull().sum()

# To display no. of samples on each class.
df['Species'].value_counts()

#Pie plot to show the overall types of Iris classifications
df['Species'].value_counts().plot(kind = 'pie',  autopct = '%1.1f%%', shadow = True, explode = [0.08,0.08,0.08])

#correlation matrix
df.corr()

#Correlation Heatmap
plt.figure(figsize=(9,7))
sns.heatmap(df.corr(),cmap='CMRmap',annot=True,linewidths=2)
plt.title("Correlation Graph",size=20)
plt.show()

#Label encoding for categorical variables
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Species'] = le.fit_transform(df['Species'])
df.head()

# To display no. of samples on each class.
df['Species'].unique()

# Splitting dataset 
from sklearn.model_selection import train_test_split

features = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']
X = df.loc[:, features].values   #defining the feature matrix
Y = df.Species

X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 40,random_state=0)

X_Train.shape

X_Test.shape

Y_Train.shape

Y_Test.shape

# Feature Scaling to bring all the variables in a single scale.
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_Train = sc.fit_transform(X_Train)
X_Test = sc.transform(X_Test)


# Importing some metrics for evaluating  models.
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import  classification_report
from sklearn.metrics import confusion_matrix

#Model Creation
#Logistic Regression
from sklearn.linear_model import LogisticRegression
log_model= LogisticRegression(random_state = 0)
log_model.fit(X_Train, Y_Train)

# model training
log_model.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_log_res=log_model.predict(X_Test)

Y_Pred_Test_log_res

print("Accuracy:",metrics.accuracy_score(Y_Test, Y_Pred_Test_log_res)*100)

print(classification_report(Y_Test, Y_Pred_Test_log_res))

confusion_matrix(Y_Test,Y_Pred_Test_log_res )

# Importing KNeighborsClassifier from sklearn.neighbors library
from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm='auto')

# Importing KNeighborsClassifier 
from sklearn.neighbors import KNeighborsClassifier
knn_model = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)

# model training
knn_model.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_knn=knn_model.predict(X_Test)

# model training
log_model.fit(X_Train, Y_Train)

Y_Pred_Test_knn

print("Accuracy:",metrics.accuracy_score(Y_Test,Y_Pred_Test_knn)*100)

print(classification_report(Y_Test,Y_Pred_Test_knn))

confusion_matrix(Y_Test, Y_Pred_Test_knn)

# Importing DecisionTreeClassifier from sklearn.tree library and creating an object of it  with hyper parameters criterion,splitter and max_depth.
from sklearn.tree import DecisionTreeClassifier
dec_tree = DecisionTreeClassifier(criterion='entropy',splitter='best',max_depth=6)

# model training
dec_tree.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_dtr=dec_tree.predict(X_Test)

Y_Pred_Test_dtr

print("Accuracy:",metrics.accuracy_score(Y_Test, Y_Pred_Test_dtr)*100)

print(classification_report(Y_Test, Y_Pred_Test_dtr))

confusion_matrix(Y_Test, Y_Pred_Test_dtr)

#Naive Bayes
from sklearn.naive_bayes import GaussianNB
nav_byes = GaussianNB()

# model training
nav_byes.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_nvb=nav_byes.predict(X_Test)

Y_Pred_Test_nvb

print("Accuracy:",metrics.accuracy_score(Y_Test, Y_Pred_Test_nvb)*100)

print(classification_report(Y_Test, Y_Pred_Test_nvb))

confusion_matrix(Y_Test,Y_Pred_Test_nvb )

#Random Forest Classification
from sklearn.ensemble import RandomForestClassifier
Ran_for = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')

# model training
Ran_for.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_rf=Ran_for.predict(X_Test)

Y_Pred_Test_rf

print("Accuracy:",metrics.accuracy_score(Y_Test,Y_Pred_Test_rf)*100)

print(classification_report(Y_Test, Y_Pred_Test_rf))

confusion_matrix(Y_Test,Y_Pred_Test_rf )

# Importing SVC from sklearn.svm library

from sklearn.svm import SVC
svm_model=SVC(C=500, kernel='rbf')

# model training
svm_model.fit(X_Train, Y_Train)

# Predicting
Y_Pred_Test_svm=svm_model.predict(X_Test)

Y_Pred_Test_svm

print("Accuracy:",metrics.accuracy_score(Y_Test,Y_Pred_Test_svm)*100)

print(classification_report(Y_Test, Y_Pred_Test_svm))

confusion_matrix(Y_Test,Y_Pred_Test_svm )

#Model Evaluation Results
print("Accuracy of Logistic Regression Model:",metrics.accuracy_score(Y_Test, Y_Pred_Test_log_res)*100)
print("Accuracy of KNN Model:",metrics.accuracy_score(Y_Test,Y_Pred_Test_knn)*100)
print("Accuracy of Decision Tree Model:",metrics.accuracy_score(Y_Test, Y_Pred_Test_dtr)*100)
print("Accuracy of Naive Bayes Model:",metrics.accuracy_score(Y_Test, Y_Pred_Test_nvb)*100)
print("Accuracy of Random Forest Classification Model:",metrics.accuracy_score(Y_Test,Y_Pred_Test_rf)*100)
print("Accuracy of SVM Model:",metrics.accuracy_score(Y_Test,Y_Pred_Test_svm)*100)

"""# **Conclusion**

*   Our dataset was not very large and consisted of only 150 rows, with all the 3 species uniformly distributed.
*   PetalWidthCm was highly correlated with PetalLengthCm

*   PetalLengthCm was highly correlated with PetalWidthCm
*   Tried with 6 different machine learning Classification models on the Iris Test data set to classify the flower into it's three species:

      a) Iris Setosa

      b) Iris Versicolour

      c) Iris Virginica,

      based on the length and width of the flower's Petals and Sepals.â€‹
 
*   We got very high accuracy score for all the models, and even the accuracy score of 100 for KNN and SVM with Linear Kernel models with some hyper parameter tuning maybe due to small size of dataset.List item



"""